{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count   \n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multiprocessing parallel version \n",
    "\n",
    "def load_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Fetches the content of a text file from a given URL.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error if the request fails\n",
    "    return response.text\n",
    "  \n",
    "# Fetch Books\n",
    "book_1 = load_text_from_url(\"https://raw.githubusercontent.com/MonaTlili/GIK2Q3_Assignment1/main/Around%20the%20World%20in%20Eighty%20Days.txt\")\n",
    "book_2 = load_text_from_url(\"https://raw.githubusercontent.com/MonaTlili/GIK2Q3_Assignment1/main/A%20Journey%20to%20the%20Centre%20of%20the%20Earth.txt\")\n",
    "book_3 = load_text_from_url(\"https://raw.githubusercontent.com/MonaTlili/GIK2Q3_Assignment1/main/Twenty%20Thousand%20Leagues%20under%20the%20Sea.txt\")\n",
    "\n",
    "# Put books into list\n",
    "books = [book_1, book_2, book_3]\n",
    "\n",
    "# List for holding cleaned books\n",
    "cleaned_books = []\n",
    "\n",
    "# Split the texts in the books into sentences for smaller chunks of data and store them in the list\n",
    "for book in books:\n",
    "    book = book.replace('!', '.').replace('?', '.')\n",
    "    book = book.split('.')\n",
    "    cleaned_books.append(book)\n",
    "    \n",
    "# Use list comprehension to create one list with all the cleaned chunks from all books\n",
    "cleaned_chunks = [chunk for book in cleaned_books for chunk in book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misra-Gries Algorithm for Word Count\n",
    "def misra_gries_word_count(words, k):\n",
    "    \"\"\"\n",
    "    Misra-Gries algorithm to count word frequencies approximately.\n",
    "\n",
    "    :param words: List of words to process\n",
    "    :param k: Parameter determining the number of counters (frequency threshold)\n",
    "    :return: Approximate word counts as a dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    counters = {}\n",
    "\n",
    "    # First pass: count words with a maximum of k-1 counters\n",
    "    for word in words:\n",
    "        if word in counters:\n",
    "            counters[word] += 1\n",
    "        elif len(counters) < k - 1:\n",
    "            counters[word] = 1\n",
    "        else:\n",
    "            # Decrement all counters if a new word can't be added\n",
    "            for key in list(counters.keys()):\n",
    "                counters[key] -= 1\n",
    "                if counters[key] == 0:\n",
    "                    del counters[key]\n",
    "\n",
    "    # Second pass: refine the counts for the words in the counters\n",
    "    refined_counts = {word: 0 for word in counters}\n",
    "    for word in words:\n",
    "        if word in refined_counts:\n",
    "            refined_counts[word] += 1\n",
    "\n",
    "    return refined_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting and processing the chunks for the Misra-Gries algortihm\n",
    "# Needs to be defines like this so that eac core can use this function, can't be a lambda function\n",
    "def process_chunk_for_misra_gries(chunk, k):\n",
    "    # split chunks into words\n",
    "    words = chunk.split()\n",
    "    return misra_gries_word_count(words, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel MapReduce implementation\n",
    "def parallel_wordcount(text_chunks):\n",
    "    k = 3\n",
    "     # Start timer\n",
    "    start_time = time.perf_counter() \n",
    "    \n",
    "    with Pool(cpu_count()) as pool:\n",
    "        # Run Misra-Gries on each chunk in parallel\n",
    "        word_counts = pool.starmap(process_chunk_for_misra_gries, [(chunk, k) for chunk in text_chunks])\n",
    "    \n",
    "    # Combine results from all chunks into a single dictionary\n",
    "    combined_counts = Counter()\n",
    "    for count in word_counts:\n",
    "        combined_counts.update(count)\n",
    "    \n",
    "    # Sort and select the top 20 words\n",
    "    top_20_counts = combined_counts.most_common(20)\n",
    "\n",
    "    # Stop timer\n",
    "    end_time = time.perf_counter()  \n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Measure performance\n",
    "    # Top 20 words \n",
    "    print(\"Top 20 Words (Parallel Word Count):\")\n",
    "    for word, count in top_20_counts:\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(f\"\\nParallel Duration: {duration:.6f} seconds\")\n",
    "\n",
    "    return dict(top_20_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run program and measure performance\n",
    "\n",
    "parallel_counts = parallel_wordcount(cleaned_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
